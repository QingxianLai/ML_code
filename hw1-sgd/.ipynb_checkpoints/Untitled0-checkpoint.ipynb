{
 "metadata": {
  "name": "",
  "signature": "sha256:b06210ae6490d278422bb2e16fd31ade1ab70c7a401b5706b91a6b77da3b386d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.cross_validation import train_test_split\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_normalization(train, test):\n",
      "    \"\"\"Rescale the data so that each feature in the training set is in\n",
      "    the interval [0,1], and apply the same transformations to the test\n",
      "    set, using the statistics computed on the training set.\n",
      "    \n",
      "    Args:\n",
      "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
      "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
      "    Returns:\n",
      "        train_normalized - training set after normalization\n",
      "        test_normalized  - test set after normalization\n",
      "\n",
      "    \"\"\"\n",
      "    lmax = train.max(axis=0)\n",
      "    lmin = train.min(axis=0)\n",
      "    lrange = lmax - lmin\n",
      "#     l = [lrange == 0.,lrange != 0]\n",
      "#     choice = [1,lrange]\n",
      "#     lrange = np.select(l,choice)\n",
      "    train_normalized = (train-lmin)/lrange     # use broadcasting here\n",
      "    test_normalized = (test-lmin)/lrange\n",
      "    return train_normalized, test_normalized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_square_loss(X, y, theta):\n",
      "    \"\"\"\n",
      "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D array of size (num_features)\n",
      "    \n",
      "    Returns:\n",
      "        loss - the square loss, scalar\n",
      "    \"\"\"\n",
      "    loss = 0       #initialize the square_loss\n",
      "    m = X.shape[0]\n",
      "    theta = theta.reshape(theta.shape[0], 1)\n",
      "\n",
      "    temp = X.dot(theta)-y.reshape(m, 1)\n",
      "    loss = (temp.T.dot(temp))/float(2*m)\n",
      "    return loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_square_loss_gradient(X, y, theta):\n",
      "    \"\"\"\n",
      "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "    \n",
      "    Returns:\n",
      "        grad - gradient vector, 1D numpy array of size (num_features)  1 X 51\n",
      "    \"\"\"\n",
      "    m = X.shape[0]\n",
      "    theta = theta.reshape(theta.shape[0],1)\n",
      "    grad = ((X.dot(theta)-y.reshape(m, 1)).T.dot(X))/float(m)\n",
      "    return grad\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4): \n",
      "    \"\"\"Implement Gradient Checker\n",
      "    Check that the function compute_square_loss_gradient returns the\n",
      "    correct gradient for the given X, y, and theta.\n",
      "\n",
      "    Let d be the number of features. Here we numerically estimate the\n",
      "    gradient by approximating the directional derivative in each of\n",
      "    the d coordinate directions: \n",
      "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1) \n",
      "\n",
      "    The approximation for the directional derivative of J at the point\n",
      "    theta in the direction e_i is given by: \n",
      "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
      "\n",
      "    We then look at the Euclidean distance between the gradient\n",
      "    computed using this approximation and the gradient computed by\n",
      "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
      "    distance exceeds tolerance, we say the gradient is incorrect.\n",
      "\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "        epsilon - the epsilon used in approximation\n",
      "        tolerance - the tolerance error\n",
      "    \n",
      "    Return:\n",
      "        A boolean value indicate whether the gradient is correct or not\n",
      "\n",
      "    \"\"\"\n",
      "    true_gradient = compute_square_loss_gradient(X, y, theta) #the true gradient  1x51\n",
      "    num_features = theta.shape[0]\n",
      "    e = np.identity(num_features)\n",
      "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
      "    for i in range(num_features):\n",
      "        test_theta_plus = theta+e[i]*epsilon\n",
      "        test_theta_minus = theta-e[i]*epsilon\n",
      "        approx_grad[i] = (compute_square_loss(X, y, test_theta_plus)-compute_square_loss(X, y, test_theta_minus))/(2*epsilon)\n",
      "    approx_grad = np.array(approx_grad).reshape(true_gradient.shape)\n",
      "    E_distance = (approx_grad-true_gradient).dot((approx_grad-true_gradient).T)\n",
      "    return E_distance <= tolerance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def batch_grad_descent(X, y, alpha=0.1, num_iter=1000, grad_checkerr=False):\n",
      "    \"\"\"\n",
      "    In this question you will implement batch gradient descent to\n",
      "    minimize the square loss objective\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - step size in gradient descent\n",
      "        num_iter - number of iterations to run \n",
      "        grad_checker - a boolean value indicating whether checking the gradient when updating\n",
      "        \n",
      "    Returns:\n",
      "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features) \n",
      "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
      "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1) \n",
      "    \"\"\"\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta_hist = np.zeros((num_iter+1, num_features))   #Initialize theta_hist\n",
      "    loss_hist = np.zeros(num_iter+1)                    #initialize loss_hist\n",
      "    theta = np.zeros(num_features)                       #initialize theta\n",
      "    theta_hist[0] = theta\n",
      "    loss_hist[0] = compute_square_loss(X, y, theta)\n",
      "    for i in range(1,num_iter+1) :\n",
      "        theta_hist[i]=theta_hist[i-1]-alpha*compute_square_loss_gradient(X,y,theta_hist[i-1])\n",
      "        grad_checkerr = grad_checker(X, y, theta_hist[i])\n",
      "        if grad_checkerr == False:\n",
      "            # raise Exception(\"do not pass the gradient test on the iteration %s\" % (i))\n",
      "            print \"grad check failed at iteration %s\" % (i)\n",
      "        loss_hist[i] = compute_square_loss(X, y, theta_hist[i])\n",
      "        # if i%100 == 0 :\n",
      "        #     print i\n",
      "        #     print loss_hist[i]\n",
      "        #     print theta_hist[i]\n",
      "        # if np.abs(loss_hist[i]-loss_hist[i-1])<0.0001:\n",
      "        #     print i\n",
      "        #     break\n",
      "\n",
      "    return theta_hist, loss_hist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def batch_grad_descent_backtracking_line(X, y, alpha_init, num_iter=1000, grad_checkerr=False):\n",
      "    \"\"\"\n",
      "    implement Backtracking-Armijo line search to improve the batch gradient desent\n",
      "\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        num_iter - number of iterations to run\n",
      "        grad_checkerr - a boolean value indicating whether checking the gradient when updating\n",
      "\n",
      "    Returns:\n",
      "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features)\n",
      "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
      "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1)\n",
      "    \"\"\"\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta_hist = np.zeros((num_iter+1, num_features))   #Initialize theta_hist\n",
      "    loss_hist = np.zeros(num_iter+1)                    #initialize loss_hist\n",
      "    theta = np.zeros(num_features)                       #initialize theta\n",
      "    theta_hist[0] = theta\n",
      "    loss_hist[0] = compute_square_loss(X, y, theta)\n",
      "    beta = 1e-4\n",
      "    c = 0.5\n",
      "    for i in range(1, num_iter+1):\n",
      "        alpha = alpha_init\n",
      "        last_loss = compute_square_loss(X,y,theta_hist[i-1])\n",
      "        last_direction = compute_square_loss_gradient(X,y,theta_hist[i-1])\n",
      "        theta_hist[i] = theta_hist[i-1]-alpha*last_direction\n",
      "\n",
      "        while compute_square_loss(X,y,theta_hist[i]) > last_loss - alpha*beta*last_direction.dot(last_direction.T):\n",
      "            alpha = alpha * c\n",
      "            theta_hist[i] = theta_hist[i-1]-alpha*compute_square_loss_gradient(X,y,theta_hist[i-1])\n",
      "\n",
      "        # grad_checkerr = grad_checker(X, y, theta_hist[i])\n",
      "        # if grad_checkerr == False:\n",
      "        #     # raise Exception(\"do not pass the gradient test on the iteration %s\" % (i))\n",
      "        #     print \"grad check failed at iteration %s\" % (i)\n",
      "        loss_hist[i] = compute_square_loss(X, y, theta_hist[i])\n",
      "\n",
      "    return theta_hist, loss_hist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
      "    \"\"\"\n",
      "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "        lambda_reg - the regularization coefficient\n",
      "    \n",
      "    Returns:\n",
      "        grad - gradient vector, 1D numpy array of size (num_features)\n",
      "    \"\"\"\n",
      "    m = X.shape[0]   # num_instances\n",
      "    y = y.reshape((m, 1))\n",
      "    theta = theta.reshape(theta.shape[0], 1)\n",
      "\n",
      "    grad = (1/float(m))*(X.dot(theta)-y).T.dot(X)+2*lambda_reg*theta.T\n",
      "    return grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_regularized_square_loss(X,y,theta,lambda_reg):\n",
      "    \"\"\"\n",
      "    Compute the loss of L2-regularized square loss function given X, y and theta\n",
      "\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
      "        lambda_reg - the regularization coefficient\n",
      "\n",
      "    Returns:\n",
      "        loss - the square loss, scalar\n",
      "    \"\"\"\n",
      "    m = X.shape[0]\n",
      "    theta = theta.reshape(theta.shape[0], 1)\n",
      "    y = y.reshape(m,1)\n",
      "    temp = X.dot(theta)-y\n",
      "    loss = (temp.T.dot(temp))/float(2*m)+lambda_reg*theta.T.dot(theta)\n",
      "    return loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
      "    \"\"\"\n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - step size in gradient descent\n",
      "        lambda_reg - the regularization coefficient\n",
      "        numIter - number of iterations to run \n",
      "        \n",
      "    Returns:\n",
      "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features) \n",
      "        loss_hist - the history of regularized loss value, 1D numpy array\n",
      "        i - the position of the result in the hist_arrays\n",
      "    \"\"\"\n",
      "\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta_hist = np.zeros((num_iter+1, num_features))   #Initialize theta_hist\n",
      "    loss_hist = np.zeros(num_iter+1)                    #initialize loss_hist\n",
      "    theta = np.ones(num_features)                       #initialize theta\n",
      "    theta_hist[0] = theta\n",
      "    loss_hist[0] = compute_regularized_square_loss(X, y, theta,lambda_reg)\n",
      "    for i in range(1,num_iter+1) :\n",
      "        theta_hist[i]=theta_hist[i-1]-alpha*compute_regularized_square_loss_gradient(X,y,theta_hist[i-1],lambda_reg)\n",
      "#         grad_checkerr = grad_checker(X, y, theta_hist[i])\n",
      "#         if grad_checkerr == False:\n",
      "#             # raise Exception(\"do not pass the gradient test on the iteration %s\" % (i))\n",
      "#             print \"grad check failed at iteration %s\" % (i)\n",
      "        loss_hist[i] = compute_square_loss(X, y, theta_hist[i])\n",
      "        # if i%100 == 0 :\n",
      "        #     print i\n",
      "        #     print loss_hist[i]\n",
      "        #     print theta_hist[i]\n",
      "        if np.abs(loss_hist[i]-loss_hist[i-1])<0.0001:\n",
      "            break\n",
      "\n",
      "    return theta_hist, loss_hist,i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('loading the dataset')\n",
      "    \n",
      "df = pd.read_csv('hw1-data.csv', delimiter=',')\n",
      "X = df.values[:, :-1] \n",
      "y = df.values[:, -1]\n",
      "\n",
      "print('Split into Train and Test')\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
      "\n",
      "X_train, X_test = feature_normalization(X_train, X_test)\n",
      "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))     #Add bias term\n",
      "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))        #Add bias term"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading the dataset\n",
        "Split into Train and Test\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta_hist, loss_hist = batch_grad_descent_backtracking_line(X_train, y_train, alpha_init=0.5)\n",
      "plt.figure()\n",
      "# plt.plot(loss_hist)\n",
      "# theta_hist, loss_hist = batch_grad_descent(X_train, y_train, 0.05)\n",
      "plt.plot(loss_hist, color='red')\n",
      "plt.xlim((0, 1000))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1152e0ad0>"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# search for lambda\n",
      "\n",
      "La = [10**k for k in range(-3,-1)]\n",
      "La = [k*10**(-3) for k in range(1,20)]\n",
      "train_loss = []\n",
      "theta_list = []\n",
      "validation_loss = []\n",
      "for lam in La:\n",
      "    theta_hist,loss_hist,pos = regularized_grad_descent(X_train,y_train,alpha=0.1,lambda_reg=lam)\n",
      "    train_loss.append(compute_square_loss(X_train,y_train,theta_hist[pos])[0,0])\n",
      "    theta_list.append(theta_hist[pos])\n",
      "    validation_loss.append(compute_square_loss(X_test,y_test,theta_hist[pos])[0,0])\n",
      "La = np.array(La)\n",
      "train_loss = np.array(train_loss)\n",
      "theta_list = np.array(theta_list)\n",
      "validation_loss = np.array(validation_loss)\n",
      "\n",
      "x = np.log10(La)\n",
      "x = La*1e+3\n",
      "plt.plot(x,train_loss,'r',label='train_loss')\n",
      "plt.plot(x,validation_loss,'b',label = 'val_loss')\n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x11572f210>"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('loading the dataset')\n",
      "    \n",
      "df = pd.read_csv('hw1-data.csv', delimiter=',')\n",
      "X = df.values[:, :-1] \n",
      "y = df.values[:, -1]\n",
      "\n",
      "print('Split into Train and Test')\n",
      "X_train_o, X_test_o, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
      "X_train_o, X_test_o = feature_normalization(X_train_o, X_test_o)\n",
      "\n",
      "\n",
      "lam = 0.007\n",
      "B_list = [2**x for x in range(11)]\n",
      "train_loss = []\n",
      "theta_list = []\n",
      "validation_loss = []\n",
      "for B in B_list:\n",
      "    X_train = np.hstack((X_train_o, B*np.ones((X_train_o.shape[0], 1))))     #Add bias term\n",
      "    X_test = np.hstack((X_test_o, B*np.ones((X_test_o.shape[0], 1))))        #Add bias term\n",
      "    theta_hist,loss_hist,pos = regularized_grad_descent(X_train,y_train,alpha=0.1,lambda_reg=0.007)\n",
      "    train_loss.append(compute_square_loss(X_train,y_train,theta_hist[pos])[0,0])\n",
      "    theta_list.append(theta_hist[pos])\n",
      "    validation_loss.append(compute_square_loss(X_test,y_test,theta_hist[pos])[0,0])\n",
      "La = np.array(La)\n",
      "train_loss = np.array(train_loss)\n",
      "theta_list = np.array(theta_list)\n",
      "validation_loss = np.array(validation_loss)\n",
      "\n",
      "x = np.log2(B_list)\n",
      "plt.plot(x,train_loss,'r',label='train_loss')\n",
      "plt.plot(x,validation_loss,'b',label = 'val_loss')\n",
      "plt.legend()\n",
      "plt.show() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading the dataset\n",
        "Split into Train and Test\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntensScMAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1152e3ad0>"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "validation_loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "array([  1.23846440e+000,   9.16035295e+178,               nan,\n",
        "                     nan,               nan,               nan,\n",
        "                     nan,               nan,               nan,\n",
        "                     nan,               nan])"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "La = [k*10**(-3) for k in range(1,100)]\n",
      "La"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "[0.001,\n",
        " 0.002,\n",
        " 0.003,\n",
        " 0.004,\n",
        " 0.005,\n",
        " 0.006,\n",
        " 0.007,\n",
        " 0.008,\n",
        " 0.009000000000000001,\n",
        " 0.01,\n",
        " 0.011,\n",
        " 0.012,\n",
        " 0.013000000000000001,\n",
        " 0.014,\n",
        " 0.015,\n",
        " 0.016,\n",
        " 0.017,\n",
        " 0.018000000000000002,\n",
        " 0.019,\n",
        " 0.02,\n",
        " 0.021,\n",
        " 0.022,\n",
        " 0.023,\n",
        " 0.024,\n",
        " 0.025,\n",
        " 0.026000000000000002,\n",
        " 0.027,\n",
        " 0.028,\n",
        " 0.029,\n",
        " 0.03,\n",
        " 0.031,\n",
        " 0.032,\n",
        " 0.033,\n",
        " 0.034,\n",
        " 0.035,\n",
        " 0.036000000000000004,\n",
        " 0.037,\n",
        " 0.038,\n",
        " 0.039,\n",
        " 0.04,\n",
        " 0.041,\n",
        " 0.042,\n",
        " 0.043000000000000003,\n",
        " 0.044,\n",
        " 0.045,\n",
        " 0.046,\n",
        " 0.047,\n",
        " 0.048,\n",
        " 0.049,\n",
        " 0.05,\n",
        " 0.051000000000000004,\n",
        " 0.052000000000000005,\n",
        " 0.053,\n",
        " 0.054,\n",
        " 0.055,\n",
        " 0.056,\n",
        " 0.057,\n",
        " 0.058,\n",
        " 0.059000000000000004,\n",
        " 0.06,\n",
        " 0.061,\n",
        " 0.062,\n",
        " 0.063,\n",
        " 0.064,\n",
        " 0.065,\n",
        " 0.066,\n",
        " 0.067,\n",
        " 0.068,\n",
        " 0.069,\n",
        " 0.07,\n",
        " 0.07100000000000001,\n",
        " 0.07200000000000001,\n",
        " 0.073,\n",
        " 0.074,\n",
        " 0.075,\n",
        " 0.076,\n",
        " 0.077,\n",
        " 0.078,\n",
        " 0.079,\n",
        " 0.08,\n",
        " 0.081,\n",
        " 0.082,\n",
        " 0.083,\n",
        " 0.084,\n",
        " 0.085,\n",
        " 0.08600000000000001,\n",
        " 0.08700000000000001,\n",
        " 0.088,\n",
        " 0.089,\n",
        " 0.09,\n",
        " 0.091,\n",
        " 0.092,\n",
        " 0.093,\n",
        " 0.094,\n",
        " 0.095,\n",
        " 0.096,\n",
        " 0.097,\n",
        " 0.098,\n",
        " 0.099]"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stochastic_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
      "    \"\"\"\n",
      "    In this question you will implement stochastic gradient descent with a regularization term\n",
      "    \n",
      "    Args:\n",
      "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
      "        y - the label vector, 1D numpy array of size (num_instances)\n",
      "        alpha - string or float. step size in gradient descent\n",
      "                NOTE: In SGD, it's not always a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
      "                if alpha is a float, then the step size in every iteration is alpha.\n",
      "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
      "                if alpha == \"1/t\", alpha = 1/t\n",
      "        lambda_reg - the regularization coefficient\n",
      "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
      "    \n",
      "    Returns:\n",
      "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features) \n",
      "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
      "    \"\"\"\n",
      "    num_instances, num_features = X.shape[0], X.shape[1]\n",
      "    theta = np.zeros(num_features) #Initialize theta\n",
      "    rand = np.arange(num_instances)\n",
      "    theta_hist = []\n",
      "    loss_hist = []\n",
      "    for i in range(num_iter):\n",
      "        theta_list = []\n",
      "        loss_list = []\n",
      "        np.random.shuffle(rand)\n",
      "        for j in range(num_instances):\n",
      "            x = X[i]\n",
      "            y = y[i]\n",
      "            theta = theta - alpha * sgd_loss_gradient(x,y,theta,lambda_reg)\n",
      "            theta_list.append(theta)\n",
      "            loss = sgd_loss(x,y,theta)\n",
      "            loss_list.append(loss)\n",
      "        theta_hist.append(theta_list)\n",
      "        loss_hist.append(loss_list)\n",
      "        if np.linalg.norm(loss_hist[i]-loss_hist[i-1])< 0.1:\n",
      "            break\n",
      "    theta_hist = np.array(theta_hist)\n",
      "    print theta_hist.shape\n",
      "    loss_hist = np.array(loss_hist)\n",
      "    print loss_hist.shape\n",
      "    return theta_hist, loss_hist\n",
      "    \n",
      "#     theta_hist = np.zeros((num_iter, num_instances, num_features))  #Initialize theta_hist\n",
      "#     loss_hist = np.zeros((num_iter, num_instances)) #Initialize loss_hist\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_loss(x,y,theta):\n",
      "    \"\"\"\n",
      "    input :\n",
      "        x - 1 X num of features array\n",
      "        y - scalar\n",
      "        theta - num of features X 1 array\n",
      "    \n",
      "    return:\n",
      "        loss - scalar\n",
      "    \"\"\"\n",
      "    \n",
      "    loss = (x.dot(theta)-y)**2\n",
      "    return loss\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_loss_gradient(x,y,theta,lambda_reg):\n",
      "    \"\"\"\n",
      "    input :\n",
      "\n",
      "        x - 1 X num of features array\n",
      "        y - scalar\n",
      "        theta - num of features X 1 array\n",
      "        lambda - regularing parameter\n",
      "    \n",
      "    return :\n",
      "        grad - 1 X num of features array\n",
      "    \"\"\"\n",
      "    \n",
      "    grad =2 * (x.dot(theta)-y)*x - lambda_reg*theta\n",
      "    return grad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('loading the dataset')\n",
      "    \n",
      "df = pd.read_csv('hw1-data.csv', delimiter=',')\n",
      "X = df.values[:, :-1] \n",
      "y = df.values[:, -1]\n",
      "\n",
      "print('Split into Train and Test')\n",
      "X_train_o, X_test_o, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
      "X_train_o, X_test_o = feature_normalization(X_train_o, X_test_o)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading the dataset\n",
        "Split into Train and Test\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test = feature_normalization(X_train, X_test)\n",
      "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))     #Add bias term\n",
      "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))        #Add bias term\n",
      "lam = 0.007\n",
      "theta_hist,loss_hist = stochastic_grad_descent(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'arrange'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-122-00adfbe92da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m#Add bias term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtheta_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_grad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-120-1b33ebae7c31>\u001b[0m in \u001b[0;36mstochastic_grad_descent\u001b[0;34m(X, y, alpha, lambda_reg, num_iter)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnum_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Initialize theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtheta_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'arrange'"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}